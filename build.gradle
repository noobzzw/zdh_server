group 'com.zyc'
version '1.0-SNAPSHOT'

wrapper {
    gradleVersion = '4.10.2' //version required
}

def scalaVersion = '2.11.12'
def sparkVersion = "2.4.4"
ext {
    baseScala = scalaVersion.substring(0, scalaVersion.lastIndexOf("."))
}

apply plugin: 'java'
apply plugin: 'scala'
apply plugin: 'idea'
apply plugin: 'application'

sourceCompatibility = 1.8

repositories {
    maven {
        url 'https://maven.aliyun.com/nexus/content/groups/public/'
    }
//    maven {
//        url ' http://repo.hortonworks.com/content/groups/public/'
//    }
    mavenCentral()
}

dependencies {
    testImplementation "org.scalatest:scalatest_$baseScala:3.0.5"
    testImplementation "org.scala-lang.modules:scala-xml_$baseScala:1.1.0"
    testImplementation 'junit:junit:4.13.2'
    testRuntimeOnly 'org.pegdown:pegdown:1.6.0'

    implementation files('libs/ojdbc6.jar')
   // implementation files('libs/hive-warehouse-connector-assembly-1.0.0.3.1.0.0-78.jar')

    implementation "org.scala-lang:scala-library:$scalaVersion"
    implementation "org.scala-lang:scala-reflect:$scalaVersion"

    //spark
    implementation (group: 'org.apache.spark', name: "spark-core_$baseScala", version: "$sparkVersion"){
        exclude(module: 'jackson-databind')
        exclude(module: 'jackson-module-scala_2.11')
    }

    implementation group: 'com.fasterxml.jackson.core', name: 'jackson-databind', version: '2.9.2'
    implementation group: 'com.fasterxml.jackson.module', name: 'jackson-module-scala_2.11', version: '2.9.2'

    implementation group: 'com.alibaba', name: 'fastjson', version: '1.2.47'
    implementation group: 'com.google.code.gson', name: 'gson', version: '2.8.2'

    implementation(group: 'org.apache.spark', name: "spark-sql_$baseScala", version: "$sparkVersion") {
        exclude(module: 'commons-compiler')
    }
    implementation group: 'org.codehaus.janino', name: 'commons-compiler', version: '3.0.8'

    implementation group: 'org.apache.spark', name: "spark-hive-thriftserver_$baseScala", version: "$sparkVersion"

    implementation group: 'org.apache.spark', name: "spark-hive_$baseScala", version: "$sparkVersion"
    implementation group: 'org.apache.hive', name: 'hive-jdbc', version: '1.2.1'
    implementation (group: 'org.spark-project.hive', name: 'hive-jdbc', version: '1.2.1.spark2'){
        exclude(module: 'servelt-api')
    }

    implementation group: 'org.apache.spark', name: "spark-streaming_$baseScala", version: "$sparkVersion"
    implementation group: 'org.apache.spark', name: "spark-streaming-kafka-0-10_$baseScala", version: "$sparkVersion"

    // https://mvnrepository.com/artifact/org.apache.spark/spark-mllib
    implementation group: 'org.apache.spark', name: 'spark-mllib_2.11', version: "$sparkVersion"


    // https://mvnrepository.com/artifact/com.crealytics/spark-excel
    implementation group: 'com.crealytics', name: 'spark-excel_2.11', version: '0.13.1'

    // https://mvnrepository.com/artifact/com.databricks/spark-xml
    implementation group: 'com.databricks', name: 'spark-xml_2.11', version: '0.9.0'

    // https://mvnrepository.com/artifact/org.elasticsearch/elasticsearch-spark-20
    implementation group: 'org.elasticsearch', name: 'elasticsearch-spark-20_2.11', version: '7.6.0'

    // https://mvnrepository.com/artifact/org.mongodb.spark/mongo-spark-connector
    implementation group: 'org.mongodb.spark', name: 'mongo-spark-connector_2.11', version: '2.4.1'

    implementation group: 'org.mongodb', name: 'mongo-java-driver', version: '3.12.2'

    //netty
    //compile group: 'mysql', name: 'mysql-connector-java', version: '5.1.46'
    implementation group: 'io.netty', name: 'netty-all', version: '4.1.23.Final'

    //json4s
    implementation group: 'org.json4s', name: "json4s-native_$baseScala", version: '3.5.4'
    implementation group: 'org.json4s', name: 'json4s-jackson_2.11', version: '3.5.4'
    
    //config
    implementation group: 'com.typesafe', name: 'config', version: '1.3.2'

    //httpclient
    implementation group: 'org.apache.httpcomponents', name: 'httpclient', version: '4.5.5'

    implementation group: 'org.apache.commons', name: 'commons-email', version: '1.2'

    //poi
    implementation group: 'org.apache.poi', name: 'poi', version: '4.0.1'
    implementation group: 'org.apache.poi', name: 'poi-ooxml', version: '4.0.1'

    //mysql
    implementation group: 'mysql', name: 'mysql-connector-java', version: '8.0.11'

    //delta
    implementation group: 'io.delta', name: 'delta-core_2.11', version: '0.5.0'

    //compile files('libs/shc-core-1.1.1-2.1-s_2.11.jar')

    // https://mvnrepository.com/artifact/com.hortonworks/shc-core
   // compile group: 'com.hortonworks', name: 'shc-core', version: '1.1.1-2.1-s_2.11'

    // https://mvnrepository.com/artifact/com.hortonworks/shc
    //compile group: 'com.hortonworks', name: 'shc', version: '1.1.1-2.1-s_2.11', ext: 'pom'


    //compile group: 'org.apache.hbase.connectors.spark', name: 'hbase-spark', version: '1.0.0'



    // https://mvnrepository.com/artifact/it.nerdammer.bigdata/spark-hbase-connector
    //compile group: 'it.nerdammer.bigdata', name: 'spark-hbase-connector_2.10', version: '1.0.3'

    //hbase
//    compile (group: 'org.apache.hbase', name: 'hbase-mapreduce', version: '2.0.0'){
//        exclude(module: 'hbase-client')
//        exclude(module: 'hbase-server')
//       // exclude(module: 'hbase-protocol')
//        exclude(module: 'hbase-common')
//        exclude(module: 'hbase-protocol-shaded')
//    }
    implementation (group: 'org.apache.hbase', name: 'hbase-client', version: '1.3.6'){
        exclude(module: 'hbase-common')
        exclude(module: 'hbase-protocol')
    }
    // https://mvnrepository.com/artifact/org.apache.hbase/hbase-common
    implementation group: 'org.apache.hbase', name: 'hbase-common', version: '1.3.6'

    implementation (group: 'org.apache.hbase', name: 'hbase-server', version: '1.3.6'){
        exclude(module: 'servlet-api-2.5')
        exclude(module: 'hbase-protocol')
        exclude(module: 'hbase-client')
        exclude(module: 'jersey-server')
        exclude(module: 'jersey-core')
    }
    // https://mvnrepository.com/artifact/org.apache.hbase/hbase-shaded-client
   // compile group: 'org.apache.hbase', name: 'hbase-shaded-client', version: '1.3.6'
    // https://mvnrepository.com/artifact/com.google.protobuf/protobuf-java
    implementation group: 'com.google.protobuf', name: 'protobuf-java', version: '2.5.0'
    // https://mvnrepository.com/artifact/org.apache.hbase.thirdparty/hbase-shaded-protobuf


    // https://mvnrepository.com/artifact/org.apache.hbase/hbase-protocol
   // compile group: 'org.apache.hbase', name: 'hbase-protocol', version: '1.3.6'

//    compile (group: 'org.apache.phoenix', name: 'phoenix-client', version: '4.14.3-HBase-1.3'){
//        exclude(module: 'phoenix-flume')
//    }
    // https://mvnrepository.com/artifact/org.apache.phoenix/phoenix-core
//    compile (group: 'org.apache.phoenix', name: 'phoenix-core', version: '4.14.3-HBase-1.3'){
//        exclude(module: 'hbase-client')
//        exclude(module: 'hbase-protocol')
//        exclude(module: 'hbase-common')
//    }
    // https://mvnrepository.com/artifact/org.apache.phoenix/phoenix-spark
//    compile (group: 'org.apache.phoenix', name: 'phoenix-spark', version: '4.14.3-HBase-1.3'){
//        exclude(module: 'hbase-client')
//        exclude(module: 'hbase-protocol')
//        exclude(module: 'hbase-common')
//    }


    implementation group: 'com.github.jsqlparser', name: 'jsqlparser', version: '3.1'

    // https://mvnrepository.com/artifact/com.redislabs/spark-redis
    implementation group: 'com.redislabs', name: 'spark-redis_2.11', version: '2.4.2'

    // https://mvnrepository.com/artifact/com.datastax.spark/spark-cassandra-connector
    implementation group: 'com.datastax.spark', name: 'spark-cassandra-connector_2.11', version: '2.4.3'

    //implementation group: 'com.springml', name: 'spark-sftp_2.11', version: '1.1.4'
    // https://mvnrepository.com/artifact/com.springml/sftp.client
    implementation group: 'com.springml', name: 'sftp.client', version: '1.0.3'


    implementation group: 'org.apache.kudu', name: 'kudu-spark2_2.11', version: '1.11.1'

    // https://mvnrepository.com/artifact/org.apache.spark/spark-streaming-flume
    implementation group: 'org.apache.spark', name: 'spark-streaming-flume_2.11', version: "$sparkVersion"

    // https://mvnrepository.com/artifact/org.apache.spark/spark-streaming-flume-sink
    implementation group: 'org.apache.spark', name: 'spark-streaming-flume-sink_2.11', version: "$sparkVersion"

    // https://mvnrepository.com/artifact/com.lucidworks.spark/spark-solr
   // implementation group: 'com.lucidworks.spark', name: 'spark-solr', version: '3.7.6'
    
    //https://github.com/apache/hudi#building-apache-hudi-from-source
    implementation group: 'org.apache.hudi', name: 'hudi-spark-bundle_2.11', version: '0.5.3'

    implementation group: 'org.apache.spark', name: 'spark-avro_2.11', version: '2.4.4'

    // https://mvnrepository.com/artifact/com.github.housepower/clickhouse-native-jdbc
    implementation group: 'com.github.housepower', name: 'clickhouse-native-jdbc', version: '2.1-stable'

    // https://mvnrepository.com/artifact/org.kie/kie-api
    implementation group: 'org.kie', name: 'kie-api', version: '7.40.0.Final'
    // https://mvnrepository.com/artifact/org.drools/drools-implementationr
    implementation (group: 'org.drools', name: 'drools-compiler', version: '7.40.0.Final'){
        exclude (module: 'protobuf-java')
    }
    // https://mvnrepository.com/artifact/org.drools/drools-core
    implementation group: 'org.drools', name: 'drools-core', version: '7.40.0.Final'

    implementation group: 'com.github.shyiko', name: 'mysql-binlog-connector-java', version: '0.21.0'

    // https://mvnrepository.com/artifact/com.pivotal/greenplum-jdbc
    implementation files('libs/greenplum-jdbc-5.1.4.jar')
    implementation files('libs/spark-greenplum-1.0-SNAPSHOT.jar')
    
    implementation group: 'com.memsql', name: 'memsql-spark-connector_2.11', version: '3.0.4-spark-2.4.4'

    // https://mvnrepository.com/artifact/org.mariadb.jdbc/mariadb-java-client
    implementation group: 'org.mariadb.jdbc', name: 'mariadb-java-client', version: '2.6.2'

    // https://mvnrepository.com/artifact/com.facebook.presto/presto-jdbc
    implementation group: 'com.facebook.presto', name: 'presto-jdbc', version: '0.240'

    // https://mvnrepository.com/artifact/com.amazon.redshift/redshift-jdbc42
    implementation files('libs/redshift-jdbc42-1.2.43.1067.jar')

    // https://mvnrepository.com/artifact/org.postgresql/postgresql
    implementation group: 'org.postgresql', name: 'postgresql', version: '42.2.16'

    // https://mvnrepository.com/artifact/org.apache.kylin/kylin-jdbc
    implementation group: 'org.apache.kylin', name: 'kylin-jdbc', version: '3.0.2'

    implementation files('libs/terajdbc4.jar')
    implementation files('libs/tdgssconfig.jar')

    implementation files('libs/kingbasejdbc3.jar')

    implementation files('libs/gbase-connector-java-8.3.81.53-build52.8-bin.jar')

    implementation files('libs/tispark-assembly-2.3.11.jar')

}


//此task 为了解决wind 下 gradle test command too long
task classpathJar(type: Jar) {

    inputs.files sourceSets.test.runtimeClasspath
    archiveName = "zyc-classpath.jar"
    doFirst {
        // If run in configuration phase, some artifacts may not exist yet (after clean)
        // and File.toURI can’t figure out what is directory to add the critical trailing slash.
        manifest {
            def classpath = sourceSets.test.runtimeClasspath.files
            attributes "Class-Path": classpath.collect {f -> f.toURI().toString()}.join(" ")
        }
    }
}

//此task 使用classpath=sourceSets.test.runtimeClasspath 会导致command too long ,原因classpath 中存在大量的路径
// 解决方案:新建一个文件zyc-classpath.jar把classpath 路径放入到此文件中,classpath 直接引用文件
task scalaTest(dependsOn: ['testClasses'], type: JavaExec) {
    main = 'org.scalatest.tools.Runner'
    //scalaTest 参数配置详解---->http://www.scalatest.org/user_guide/using_the_runner
    args = ['-R', 'build/classes/scala/test', '-w','com.zyc.zdh','-o', '-h', 'build/reports']
    classpath = classpathJar.outputs.files
}

test.dependsOn scalaTest

sourceSets {
    mainClassName='com.zyc.SystemInit'
    main {
        scala {
            srcDirs = ['src/main/scala']
        }
        resources {
            srcDir 'src/main/resources'
        }
    }
    test {
        scala {
            srcDirs = ['src/test/scala']
        }
        resources {
            srcDir 'src/main/resources' //资源目录
        }
    }
}
//
jar {
//    from {
//        //添加依懒到打包文件
//        zip64 = true
//        configurations.compile.findAll {
//                    it.name.contains("delta-core")|| it.name.contains("mysql")||
//                            it.name.contains("jsqlparser")||it.name.contains("mongo")||
//                            it.name.contains("elasticsearch")||it.name.contains("oracle")||
//                            it.name.contains("hbase")||it.name.contains("phoenix")||
//                            it.name.contains("config")||it.name.contains("cassandra")||
//                            it.name.contains("kudu")||it.name.contains("flume")||
//                            it.name.contains("ojdbc")
//        }.collect { it.isDirectory() ? it : zipTree(it) }
//    }
    manifest {
        attributes 'Main-Class': 'com.zyc.SystemInit'
    }
}
//processResources {
//    exclude { "**/*.*" }
//}

// 复制依赖jar至build/libs
task copyJar(type: Copy) {
    // from configurations.runtime
    from configurations.runtimeOnly
    into('release/libs')
    exclude 'hadoop**.jar','spark-core*','spark-catalyst*','spark-tags*','spark-network*','spark-unsafe*',
            'spark-mllib*','spark-sql*'
}

// 复制依赖jar至build/libs
task copyJar2(type: Copy) {
    // from configurations.runtimeOnly
    from ("build/libs")
    into('release/libs')
}

task copyJar3(type:Copy){
    from ("libs")
    into('release/copy_spark_jars')
}

task copyConf(type:Copy){
    from ("src/main/resources")
    into ('release/conf')
}

task clearJar(type: Delete) {
    delete 'release/libs'
}

task clearHadoopJar(type: Delete) {
    delete 'release/libs' , 'hadoop*'
    followSymlinks = true
}

// 打包
task release(type: Copy, dependsOn: [build,clearJar,jar,copyJar,copyConf,copyJar2,copyJar3])


configurations {
    jar.archiveName = "$rootProject.name" + ".jar"
}


println "Building project with Scala version $baseScala" + new Date()

println "if you use gradle run this project at local ,please delete builid.gradle `processResources` configuration"
println "如果本地使用gradle 运行项目,请先删除builid.gradle中processResources相关配置,但是打包时尽量不要把配置文件打到jar中"
//必不可少-如果没有则项目本身的代码不会打进去
tasks.withType(ScalaCompile) {
    scalaCompileOptions.additionalParameters = [
            "-deprecation",
            "-unchecked",
            "-encoding", "utf8",
            "-Xlog-reflective-calls",
            "-feature",
            "-language:postfixOps",
            "-language:implicitConversions",
            "-language:existentials",
            "-Xlint:by-name-right-associative",
            "-Xlint:delayedinit-select",
            "-Xlint:doc-detached",
            "-Xlint:missing-interpolator",
            "-Xlint:nullary-override",
            "-Xlint:nullary-unit",
            "-Xlint:option-implicit",
            "-Xlint:package-object-classes",
            "-Xlint:poly-implicit-overload",
            "-Xlint:private-shadow",
            "-Xlint:stars-align",
            "-Xlint:type-parameter-shadow",
            "-Xlint:unsound-match",
    ]

    if (baseScala != '2.11') {
        scalaCompileOptions.additionalParameters += [
                "-Xlint:constant",
                "-Xlint:unused"
        ]
    }

    configure(scalaCompileOptions.forkOptions) {
        memoryMaximumSize = '2g'
        //jvmArgs = ['-Xss2m', '-XX:MaxPermSize=512m']
    }
}